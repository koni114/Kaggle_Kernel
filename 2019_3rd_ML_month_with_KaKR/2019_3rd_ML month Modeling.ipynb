{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# 추가 고려 사항\n# 모델에 metrics -> f1_score 를 넣을수는 없는가? \n# metrics=['accuracy', precision, recall, f1score]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# library load\nimport gc             # garbage collection library\nimport os             # 기본적인 환경을 setting 할 수 있는 library(ex) dir 등) \nimport warnings       # warnings을 무시하기 위해 일반적으로 사용되는 library\nimport numpy as np    # numpy package : array, 배열 연산, slicing 에 자주 사용되는 library\nimport pandas as pd   # 주로 분석 모델에 사용하는 package\nfrom tqdm import tqdm # 진행바를 만들어주는 library\nfrom sklearn.model_selection import StratifiedKFold, KFold # K-fold 검증\n\nfrom keras import backend as K # 딥러닝 모델 변수들의 초기값을 setting해 주기 위한 library\nfrom keras.preprocessing.image import ImageDataGenerator # ImageDataGenerator\n\nfrom keras.applications import Xception                                        # Xception pretrained Model 사용\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  # callback 도구 : EarlyStopping, ReduceOnPlateau, ModelCheckPoint\nfrom keras import layers, models, optimizers\n\nwarnings.filterwarnings(action = 'ignore')\nK.image_data_format()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.listdir(\"../3rd-ml-month-car-image-cropping-dataset/\")  \nos.listdir(\"../input/3rd-ml-month-car-image-cropping-dataset/\")  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"DATA_PATH         = '../input/2019-3rd-ml-month-with-kakr/'  # 기본 디렉토리 설정하기 위한 변수 선언\nDATA_CROPPED_PATH = '../input/3rd-ml-month-car-image-cropping-dataset/'\nos.listdir(DATA_PATH)                                # 기본 디렉토리 설정 -> 해당 디렉토리의 file 이나 folder 확인 가능\n\n# 이미지 폴더 경로 : cropped data 사용\nTRAIN_IMG_PATH = os.path.join(DATA_CROPPED_PATH, 'train_crop')\nTEST_IMG_PATH  = os.path.join(DATA_CROPPED_PATH, 'test_crop')\n\n# CSV 파일 경로를 통해 data read : train, test, class\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test  = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))\n\ndf_train.head() # 몇개만 데이터 확인해보기 위해, head() 사용\ndf_test.head()  # test data 이기 때문에, class가 없는 것을 확인!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 지표 선택\n# 이번 컴페티션에서 f1_score가 지표.\n# f1_score를 내가 만들고자 하는 모델에 집어 넣을 수 있는가?\nfrom sklearn.metrics import f1_score\ndef micro_f1(y_true, y_pred):\n    return f1_score(y_true, y_pred, average= 'micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ImageDataGenerator를 이용하여 train, valid, test datagen 만들기.\n# Jang님 kernel 참조!\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    rotation_range       = 30,\n    width_shift_range    = 0.2,\n    height_shift_range   = 0.2,\n    horizontal_flip      = True,\n    vertical_flip        = False,\n    zoom_range           = 0.3,\n    shear_range          = 0.3,\n    # brightness_range=(1, 1.2),\n    fill_mode            = 'nearest'\n    )\n\nvalid_datagen = ImageDataGenerator(\n    rescale= 1./255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True  # divide each input by its std\n    )\ntest_datagen = ImageDataGenerator(\n    rescale= 1./255\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def call_back(model_name, patient):\n    ES = EarlyStopping(\n        monitor='val_loss', \n        patience=patient, \n        mode='min', \n        verbose=1)\n    RR = ReduceLROnPlateau(\n        monitor = 'val_loss', \n        factor = 0.5, \n        patience = patient / 2, \n        min_lr=0.000001, \n        verbose=1, \n        mode='min')\n    MC = ModelCheckpoint(\n        filepath=model_name, \n        monitor='val_loss', \n        verbose=1, \n        save_best_only=True, \n        mode='min')\n\n    return [ES, RR, MC]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델 층 설계\ndef get_model(model_name, iamge_size):\n    base_model = model_name(weights     = 'imagenet', \n                            input_shape = (iamge_size, iamge_size, 3), # Xception 모델에 맞는 input shape : 299 * 299 \n                            include_top = False\n                           )\n    \n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.25))\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.25))\n \n    model.add(layers.Dense(196, activation='softmax'))\n    model.summary()\n\n    optimizer = optimizers.RMSprop(lr=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics= ['acc'] )\n\n    return model\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['class'] = df_train['class'].astype('str')\ndf_train          = df_train[['img_file', 'class']]\ndf_test           = df_test[['img_file']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 검증 방법 선택.\n# k-fold 검증 방법을 선택.\n# 만약 9시간을 초과하는 경우, 다른 방법을 강구 해야 함. \n# -> 커널을 여러개 만들어서 각각 모델을 만들고, 결과 값을 평균 내서 제출.\n\n# 우선 sklearn 의 StratifiedKFold 함수를 이용해, k = 5 인 해당 객체 생성.\nK          = 5\nIMAGE_SIZE = 299 # Xception Model은 299가 가장 최적.\nBATCH_SIZE = 32\nEPOCH      = 20\nmodel_path = './'\nmodel_xception_names = []\n\nskf = StratifiedKFold(n_splits = K, random_state = 2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 모델 수행 : Xception Model\n\nj = 1\nmodel_xception_names = []\nfor (train_index, valid_index) in skf.split(\n    df_train['img_file'], \n    df_train['class']):\n\n    traindf = df_train.iloc[train_index, :].reset_index()\n    validdf = df_train.iloc[valid_index, :].reset_index()\n\n    print(\"=========================================\")\n    print(\"====== K Fold Validation step => %d/%d =======\" % (j , K))\n    print(\"=========================================\")\n    \n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe   = traindf,\n        directory   = TRAIN_IMG_PATH,\n        x_col       = 'img_file',\n        y_col       = 'class',\n        target_size = (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode  = 'rgb',\n        class_mode  = 'categorical',\n        batch_size  =  BATCH_SIZE,\n        seed        =  2019,\n        shuffle     = True\n        )\n\n    valid_generator = valid_datagen.flow_from_dataframe(\n        dataframe   = validdf,\n        directory   = TRAIN_IMG_PATH,\n        x_col       = 'img_file',\n        y_col       = 'class',\n        target_size = (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode  = 'rgb',\n        class_mode  ='categorical',\n        batch_size  = BATCH_SIZE,\n        seed        = 2019,\n        shuffle     = True\n        )\n\n    model_name = model_path + str(j) + '_xception.hdf5'\n    model_xception_names.append(model_name)\n    model_xception = get_model(Xception, IMAGE_SIZE)\n    \n    try:\n        model_xception.load_weights(model_name)\n    except:\n        pass\n        \n    history = model_xception.fit_generator(\n        train_generator,\n        steps_per_epoch  = len(traindf.index) / BATCH_SIZE,\n        epochs           = EPOCH,\n        validation_data  = valid_generator,\n        validation_steps = len(validdf.index) / BATCH_SIZE,\n        verbose          = 1,\n        shuffle          = False,\n        callbacks        = call_back( model_name, 6)\n        )\n        \n    j+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델 평가\n# xception_prediction  = [] # 예측 값을 저장할 list 선언\n# model_xception_names = [] # 모델 경로 + 이름을 저장할 list 선언\n# for i in range(1, 4):\n#     model_xception_names.append(model_path + str(i) + '_xception.hdf5')\n\n# test_generator 생성\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe = df_test,\n    directory = TEST_IMG_PATH,\n    x_col     = 'img_file',\n    y_col     = None,\n    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n    color_mode  = 'rgb',\n    class_mode  = None,\n    batch_size  = BATCH_SIZE,\n    shuffle     = False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, name in enumerate(model_xception_names):\n    model_xception = get_model(Xception, IMAGE_SIZE)\n    model_xception.load_weights(name)\n    test_generator.reset()\n    pred = model_xception.predict_generator(\n        generator = test_generator,\n        steps     = len(df_test) / BATCH_SIZE,\n        verbose   = 1\n    )\n    xception_prediction.append(pred)\ny_pred_xception = np.mean(xception_prediction, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_class_indices=np.argmax(y_pred_xception, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"labels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nfinal_pred = [labels[k] for k in preds_class_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nsubmission[\"class\"] = final_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}